{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f996c132",
   "metadata": {},
   "source": [
    "# Survival's probability of Titanic's passengers using Decision Trees and Random Forests\n",
    "\n",
    "Used Techniques: Supervised learning, Decision Trees and Random Forests. \n",
    "\n",
    "In this exercise, we are going to use a kaggle dataset coming from this URL: https://www.kaggle.com/code/faressayah/decision-trees-random-forest-for-beginners/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "986f9752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9f775b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since we downloaded the data into a csv, let's convert it into a pandas dataframe.\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "#We first see how our data looks like\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2555c1c",
   "metadata": {},
   "source": [
    "Before starting the analysis, the documentation highlights the following column definitions:\n",
    "- pclass: Ticket class. 1=1st, 2=2nd, 3=3rd\n",
    "- sibsp: # of siblings/spouses aboard the titanic\n",
    "- parch: # of parents/children aboard the titanic\n",
    "- fare: Passenger fare\n",
    "- Cabin: Cabin number\n",
    "- Embarked: port of embakation. C=Cherbourg, Q=Queenstown, S=Southampton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2559bd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 12)\n",
      "(418, 11)\n",
      "\n",
      " Train dataset information\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n",
      "\n",
      " Test dataset information\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  418 non-null    int64  \n",
      " 1   Pclass       418 non-null    int64  \n",
      " 2   Name         418 non-null    object \n",
      " 3   Sex          418 non-null    object \n",
      " 4   Age          332 non-null    float64\n",
      " 5   SibSp        418 non-null    int64  \n",
      " 6   Parch        418 non-null    int64  \n",
      " 7   Ticket       418 non-null    object \n",
      " 8   Fare         417 non-null    float64\n",
      " 9   Cabin        91 non-null     object \n",
      " 10  Embarked     418 non-null    object \n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 36.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Now that our data is legible our first step is knowing both of our dataframes. \n",
    "#Magnitude of or dataframe\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "\n",
    "#Information per column\n",
    "print(\"\\n Train dataset information\")\n",
    "print(train_df.info())\n",
    "print(\"\\n Test dataset information\")\n",
    "print(test_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca4f91b",
   "metadata": {},
   "source": [
    "By seeing both lists, there are several points to address: \n",
    "1. The training and testing sets the columns \"Age\" and \"Cabin\" have some null data. It's important to fix it in order to avoid any model underfitting. \n",
    "2. The columns \"Name\", \"Sex\", \"Tickets\", \"Cabin\" and \"Embarked\" are an object.This must be corrected as the models do not accept object type data.\n",
    "3. The column \"Age\" is a float type data. Which by default doesn't make any sense. \n",
    "\n",
    "Now that we know more about our data, let's dive more into the samples to correct each case.\n",
    "\n",
    "The easiest case to correct would be the \"Sex\" column. Intuitively, there must be only 2 options \"F\" or \"M\". If that's the case, we could just transform this into a binary data. If not, we must ensure we have tops 3 options, \"F\", \"M\" or \"N/A\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8d0f5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male      577\n",
      "female    314\n",
      "Name: Sex, dtype: int64\n",
      "male      266\n",
      "female    152\n",
      "Name: Sex, dtype: int64\n",
      "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Age', 'SibSp', 'Parch',\n",
      "       'Ticket', 'Fare', 'Cabin', 'Embarked', 'Sex_bool'],\n",
      "      dtype='object')\n",
      "Index(['PassengerId', 'Pclass', 'Name', 'Age', 'SibSp', 'Parch', 'Ticket',\n",
      "       'Fare', 'Cabin', 'Embarked', 'Sex_bool'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_df[\"Sex\"].value_counts()) #data label counting\n",
    "print(test_df[\"Sex\"].value_counts()) #data label counting\n",
    "\n",
    "#Since our first hypothesis was correct we can turn the data into binary (boolean).\n",
    "train_df[\"Sex_bool\"] = train_df[\"Sex\"].map({'female':1, 'male':0})\n",
    "test_df[\"Sex_bool\"] = test_df[\"Sex\"].map({'female':1, 'male':0})\n",
    "\n",
    "#We check that the changes have been correctly made. \n",
    "train_df.head(5)\n",
    "test_df.head(5)\n",
    "\n",
    "#Now we delete the object type \"Sex\" column.\n",
    "train_df = train_df.drop('Sex', axis=1)\n",
    "test_df = test_df.drop('Sex', axis=1)\n",
    "\n",
    "#We check how the dataset looks\n",
    "print(train_df.columns)\n",
    "print(test_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c42029",
   "metadata": {},
   "source": [
    "Based on the information provided by the documentation, we know that the \"Embarked\" column has 3 multiple choices that we can handle as a categorical data. However, since the goal of the project is to practice decision trees and Random Forest techniques, our best choice is to apply One-hot Encoding. We can not implement ordinal encoding, because trees tend to interpret the values as if they would have a real order, even if the categories don't have any, like this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49fb16be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Sex_bool</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>3</td>\n",
       "      <td>Spector, Mr. Woolf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A.5. 3236</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "      <td>Oliva y Ocana, Dona. Fermina</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17758</td>\n",
       "      <td>108.9000</td>\n",
       "      <td>C105</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>3</td>\n",
       "      <td>Saether, Mr. Simon Sivertsen</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SOTON/O.Q. 3101262</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>3</td>\n",
       "      <td>Ware, Mr. Frederick</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>359309</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>3</td>\n",
       "      <td>Peter, Master. Michael J</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2668</td>\n",
       "      <td>22.3583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Pclass                                          Name   Age  \\\n",
       "0            892       3                              Kelly, Mr. James  34.5   \n",
       "1            893       3              Wilkes, Mrs. James (Ellen Needs)  47.0   \n",
       "2            894       2                     Myles, Mr. Thomas Francis  62.0   \n",
       "3            895       3                              Wirz, Mr. Albert  27.0   \n",
       "4            896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  22.0   \n",
       "..           ...     ...                                           ...   ...   \n",
       "413         1305       3                            Spector, Mr. Woolf   NaN   \n",
       "414         1306       1                  Oliva y Ocana, Dona. Fermina  39.0   \n",
       "415         1307       3                  Saether, Mr. Simon Sivertsen  38.5   \n",
       "416         1308       3                           Ware, Mr. Frederick   NaN   \n",
       "417         1309       3                      Peter, Master. Michael J   NaN   \n",
       "\n",
       "     SibSp  Parch              Ticket      Fare Cabin  Sex_bool  Embarked_C  \\\n",
       "0        0      0              330911    7.8292   NaN         0           0   \n",
       "1        1      0              363272    7.0000   NaN         1           0   \n",
       "2        0      0              240276    9.6875   NaN         0           0   \n",
       "3        0      0              315154    8.6625   NaN         0           0   \n",
       "4        1      1             3101298   12.2875   NaN         1           0   \n",
       "..     ...    ...                 ...       ...   ...       ...         ...   \n",
       "413      0      0           A.5. 3236    8.0500   NaN         0           0   \n",
       "414      0      0            PC 17758  108.9000  C105         1           1   \n",
       "415      0      0  SOTON/O.Q. 3101262    7.2500   NaN         0           0   \n",
       "416      0      0              359309    8.0500   NaN         0           0   \n",
       "417      1      1                2668   22.3583   NaN         0           1   \n",
       "\n",
       "     Embarked_Q  Embarked_S  \n",
       "0             1           0  \n",
       "1             0           1  \n",
       "2             1           0  \n",
       "3             0           1  \n",
       "4             0           1  \n",
       "..          ...         ...  \n",
       "413           0           1  \n",
       "414           0           0  \n",
       "415           0           1  \n",
       "416           0           1  \n",
       "417           0           0  \n",
       "\n",
       "[418 rows x 13 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.get_dummies(train_df, columns=['Embarked'])\n",
    "train_df\n",
    "\n",
    "test_df = pd.get_dummies(test_df, columns=['Embarked'])\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96415d7d",
   "metadata": {},
   "source": [
    "Our Next columns to treat are \"Name\" and \"Tickets\". What these columns have in common is that both of them can be considered as unique ids per passenger. What we can do for now is just transform them as categorical data in case we later need to transform any of these two into numerical and meanwhile reduce the memory usage. This won't make any changes on the data apart from the data type, meaning the data will be displayed exactly the same as the beginning. As an advantage, having them as categorical data, allow us to see if they are indeed unique and make analyses related to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dd8b633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name unique count:  891\n",
      "Ticket unique count:  681\n",
      "Test Name unique count:  418\n",
      "Test ticket unique count:  363\n"
     ]
    }
   ],
   "source": [
    "train_df['Name'] = train_df['Name'].astype('category')\n",
    "train_df['Ticket'] = train_df['Ticket'].astype('category')\n",
    "\n",
    "test_df['Name'] = test_df['Name'].astype('category')\n",
    "test_df['Ticket'] = test_df['Ticket'].astype('category')\n",
    "\n",
    "#Unique values\n",
    "print(\"Name unique count: \",train_df['Name'].nunique())\n",
    "print(\"Ticket unique count: \",train_df['Ticket'].nunique())\n",
    "\n",
    "print(\"Test Name unique count: \",test_df['Name'].nunique())\n",
    "print(\"Test ticket unique count: \",test_df['Ticket'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a219e9b",
   "metadata": {},
   "source": [
    "As we can see, in the training names, we don't have any repeated data, but in the rest of counts, the ids are not unique.This might just be a characteristic of the data or maybe a clue of a estructural relationship. But we will address this possible relationship when we get to the exploratory analysis.In this part, we will only focus on getting quality data to analyze.\n",
    "\n",
    "For this next part we will approach a solution for the null data in the \"Age\" Column. For this column we can have two different solutions: \n",
    "1. We can calculate the median per class and impute the result. In one hand, this is simple, fast, the median is robust upon outliers and makes sense that the age can be correlated with socioeconomic conditions. On the other hand, this technique totally ignores important variables that can give us more accuracy on the age, like sex, number of members of the family, etc. \n",
    "2. Impute using KNN. On the bright side, this solution takes more variables into account, which lead to more personalized estimations. Besides it can capture fine patterns like the approximate age for the specific profile. Normally, this type of solutions can be much more lower to implement on huge datasets, but since our dataset is not that big we can use this solution. \n",
    "\n",
    "Both can be really good for this problem, but since our dataset is not that huge and we want to obtain a much more related value to the situation of each passenger, the most adequate solution would be number 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edbffc03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def knnimputer_misval(features, df):\n",
    "    impute_df = df[features].copy() #We create a new dataframe just with these columns.\n",
    "    \n",
    "    #Standardize the variables to avoid that a column with high values domains over the distance from other columns.\n",
    "    scaler = StandardScaler()\n",
    "    imputed_scaled = scaler.fit_transform(impute_df) \n",
    "    \n",
    "    #Apply the KNNImputer\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    imputed_array = imputer.fit_transform(imputed_scaled)\n",
    "\n",
    "    #Convert the result into a DataFrame\n",
    "    imputed_df = pd.DataFrame(imputed_array, columns=features)\n",
    "\n",
    "    #Invert the scaled DataFrame to return the original ages.\n",
    "    imputed_df_unscaled = pd.DataFrame(scaler.inverse_transform(imputed_df), columns=features)\n",
    "\n",
    "    return imputed_df_unscaled\n",
    "\n",
    "#To implement impute KNN we must select the columns that can be related with the age and that are numerical.\n",
    "features = [\"Age\", \"Pclass\", \"SibSp\", \"Parch\", \"Fare\"] #These are the most intuitive ones. \n",
    "#Age is the one to be imputed\n",
    "#Pclass refers to social status which leads to certain tendency on ages. \n",
    "#SibSp and Parch are the number of familiy members, which can be related to parents, children, spouses.\n",
    "#Fare refers to type of passenger\n",
    "\n",
    "#we calculate the KNNImputation\n",
    "imputed_df_unscaled_train = knnimputer_misval(features,train_df)\n",
    "imputed_df_unscaled_test = knnimputer_misval(features,test_df)\n",
    "\n",
    "#Replace the original ages in the original dataset\n",
    "train_df[\"Age\"] = imputed_df_unscaled_train[\"Age\"]\n",
    "test_df[\"Age\"] = imputed_df_unscaled_test[\"Age\"]\n",
    "\n",
    "#Now we check that we don't have any null data within the age column. \n",
    "train_df[\"Age\"].isnull().sum()\n",
    "test_df[\"Age\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a438d92",
   "metadata": {},
   "source": [
    "For the last column \"Cabin\" with null data, we are going to start from the hypothesis that if you have a Passenger that has a cabin, and a duplicated ticket, then we can assign the same deck. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97779740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676\n",
      "324\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#First let's see the groups of cabins we have\n",
    "train_df['Cabin'].value_counts()\n",
    "test_df['Cabin'].value_counts()\n",
    "\n",
    "#As we can see the Cabin data has 1 letter and a number, the letter refers to the deck\n",
    "#What we can do is to extract the letter and assign it to persons that have duplicated tickets.\n",
    "train_df['Cabin_deck'] = train_df['Cabin'].str[0]\n",
    "test_df['Cabin_deck'] = test_df['Cabin'].str[0]\n",
    "\n",
    "#To do this we make a dictionary that links the cabin letter with ticket id. If a ticket is duplicated with other passenger, we \n",
    "#will assume that they were on the same deck. \n",
    "\n",
    "#First we have to drop duplicates and have a clean dataset with not nan.\n",
    "ticket_to_cabin = train_df[train_df['Cabin_deck'].notna()].drop_duplicates(subset=\"Ticket\")[[\"Ticket\", \"Cabin_deck\"]]\n",
    "ticket_to_cabin_test = test_df[test_df['Cabin_deck'].notna()].drop_duplicates(subset=\"Ticket\")[[\"Ticket\", \"Cabin_deck\"]]\n",
    "\n",
    "#Once we have this, we can make our dictionary with the data that is from our interest.\n",
    "ticket_to_cabin_dict = dict(zip(ticket_to_cabin[\"Ticket\"], ticket_to_cabin[\"Cabin_deck\"]))\n",
    "ticket_to_cabin_dict_test = dict(zip(ticket_to_cabin_test[\"Ticket\"], ticket_to_cabin_test[\"Cabin_deck\"]))\n",
    "\n",
    "#Now that we have this, we can now assign the cabin to a passenger\n",
    "train_df[\"Cabin_deck\"] = train_df[\"Cabin_deck\"].fillna(train_df[\"Ticket\"].map(ticket_to_cabin_dict))\n",
    "test_df[\"Cabin_deck\"] = test_df[\"Cabin_deck\"].fillna(test_df[\"Ticket\"].map(ticket_to_cabin_dict_test))\n",
    "\n",
    "#We count the missing values\n",
    "print(train_df['Cabin_deck'].isna().sum())\n",
    "print(test_df['Cabin_deck'].isna().sum())\n",
    "\n",
    "#As we can see, the data is still having lots of nan data. \n",
    "#For those cases, we are going to use a KNNImputer.\n",
    "#But we have to have in mind that KNNImputer doesn't accept any non-numerical data.\n",
    "#Therefore, we need to first transform the Cabin_deck into a numerical data. \n",
    "#To do that we will first see how many decks and how related to the classes are. \n",
    "train_df.groupby('Pclass')[\"Cabin_deck\"].unique() \n",
    "#Since they are related in a descendant order, we can transform them as a=1, b=2, etc.\n",
    "\n",
    "deck_mapping = {chr(i):i-64 for i in range(ord('A'), ord('T')+1)}\n",
    "train_df[\"Cabin_deck_num\"] = train_df[\"Cabin_deck\"].map(deck_mapping)\n",
    "test_df[\"Cabin_deck_num\"] = test_df[\"Cabin_deck\"].map(deck_mapping)\n",
    "\n",
    "#Now we calculate de KNNImputer with these features:\n",
    "#Cabin_deck_num, PClass, SibSp and Parch and Fare. \n",
    "features = ['Cabin_deck_num','Pclass', 'SibSp', 'Parch', 'Fare']\n",
    "imputed_df_unscaled_train = knnimputer_misval(features,train_df)\n",
    "imputed_df_unscaled_test= knnimputer_misval(features,test_df)\n",
    "\n",
    "#Replace the original decks in the original dataset\n",
    "train_df[\"Cabin_deck_num\"] = imputed_df_unscaled_train[\"Cabin_deck_num\"].round()\n",
    "test_df[\"Cabin_deck_num\"] = imputed_df_unscaled_test[\"Cabin_deck_num\"].round()\n",
    "\n",
    "num_to_deck = {i:chr(i+64) for i in range(1,21)}\n",
    "train_df[\"Cabin_deck\"] = train_df[\"Cabin_deck_num\"].map(num_to_deck)\n",
    "test_df[\"Cabin_deck\"] = test_df[\"Cabin_deck_num\"].map(num_to_deck)\n",
    "\n",
    "#Validation for na values. \n",
    "print(train_df[\"Cabin_deck\"].isna().sum())\n",
    "print(test_df[\"Cabin_deck\"].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f2c767",
   "metadata": {},
   "source": [
    "Now when we see our dataframe's information, we see that the column \"Cabin\" will be the only one that will not have a full set of data, but that is because we added the columns of the deck that can replace this column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f04b2669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 16 columns):\n",
      " #   Column          Non-Null Count  Dtype   \n",
      "---  ------          --------------  -----   \n",
      " 0   PassengerId     891 non-null    int64   \n",
      " 1   Survived        891 non-null    int64   \n",
      " 2   Pclass          891 non-null    int64   \n",
      " 3   Name            891 non-null    category\n",
      " 4   Age             891 non-null    float64 \n",
      " 5   SibSp           891 non-null    int64   \n",
      " 6   Parch           891 non-null    int64   \n",
      " 7   Ticket          891 non-null    category\n",
      " 8   Fare            891 non-null    float64 \n",
      " 9   Cabin           204 non-null    object  \n",
      " 10  Sex_bool        891 non-null    int64   \n",
      " 11  Embarked_C      891 non-null    uint8   \n",
      " 12  Embarked_Q      891 non-null    uint8   \n",
      " 13  Embarked_S      891 non-null    uint8   \n",
      " 14  Cabin_deck      891 non-null    object  \n",
      " 15  Cabin_deck_num  891 non-null    float64 \n",
      "dtypes: category(2), float64(3), int64(6), object(2), uint8(3)\n",
      "memory usage: 143.5+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype   \n",
      "---  ------          --------------  -----   \n",
      " 0   PassengerId     418 non-null    int64   \n",
      " 1   Pclass          418 non-null    int64   \n",
      " 2   Name            418 non-null    category\n",
      " 3   Age             418 non-null    float64 \n",
      " 4   SibSp           418 non-null    int64   \n",
      " 5   Parch           418 non-null    int64   \n",
      " 6   Ticket          418 non-null    category\n",
      " 7   Fare            417 non-null    float64 \n",
      " 8   Cabin           91 non-null     object  \n",
      " 9   Sex_bool        418 non-null    int64   \n",
      " 10  Embarked_C      418 non-null    uint8   \n",
      " 11  Embarked_Q      418 non-null    uint8   \n",
      " 12  Embarked_S      418 non-null    uint8   \n",
      " 13  Cabin_deck      418 non-null    object  \n",
      " 14  Cabin_deck_num  418 non-null    float64 \n",
      "dtypes: category(2), float64(3), int64(5), object(2), uint8(3)\n",
      "memory usage: 66.0+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c640652",
   "metadata": {},
   "source": [
    "Since we have now a complete and nan data free Dataframe, we can now start with our EDA. \n",
    "For starters, we will beginn with a general overview of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89c6559a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  891.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   30.104231    0.523008   \n",
      "std     257.353842    0.486592    0.836071   13.734518    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   22.000000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   29.200000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   39.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare    Sex_bool  Embarked_C  Embarked_Q  Embarked_S  \\\n",
      "count  891.000000  891.000000  891.000000  891.000000  891.000000  891.000000   \n",
      "mean     0.381594   32.204208    0.352413    0.188552    0.086420    0.722783   \n",
      "std      0.806057   49.693429    0.477990    0.391372    0.281141    0.447876   \n",
      "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "25%      0.000000    7.910400    0.000000    0.000000    0.000000    0.000000   \n",
      "50%      0.000000   14.454200    0.000000    0.000000    0.000000    1.000000   \n",
      "75%      0.000000   31.000000    1.000000    0.000000    0.000000    1.000000   \n",
      "max      6.000000  512.329200    1.000000    1.000000    1.000000    1.000000   \n",
      "\n",
      "       Cabin_deck_num  \n",
      "count      891.000000  \n",
      "mean         5.084175  \n",
      "std          1.462139  \n",
      "min          1.000000  \n",
      "25%          5.000000  \n",
      "50%          6.000000  \n",
      "75%          6.000000  \n",
      "max         20.000000  \n"
     ]
    }
   ],
   "source": [
    "#Statistical description quick view\n",
    "print(train_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfceb217",
   "metadata": {},
   "source": [
    "This statistical give us generalized info like the one enlisted, but quite enough to make our own hypothesis and delimit the best features to determine a passengers survival probability:\n",
    "- Approximately 38% of passengers survived\n",
    "- Less than 25% of passengers were between classes 1 and 2. \n",
    "- 75% of passengers were on third class\n",
    "- The average age of the passengers was around 30 years old and >=75% of the passengers were aprox around this age.\n",
    "- The most popular por of embarcation was the \"s\"\n",
    "- 35% of the passengers were women. \n",
    "\n",
    "Based on this we can make some hypothesis, such as:\n",
    "1. If 38% of passengers survivied and 35% of passengers were women, if you were a women you had high chances to survive.\n",
    "2. If you were around 30 years old you had chances to survive but they were not high. \n",
    "3. If you were on third class you would likely not survive. \n",
    "4. If you were on first or second class, you had chances to survive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3749c13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Sex_bool\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac71ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
